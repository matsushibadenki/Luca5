# /env.sample

# --------------------------
# Server Configuration
# --------------------------
HOST="0.0.0.0"
PORT=8000
ANALYTICS_PORT=8001

# --------------------------
# LLM Backend Configuration
# --------------------------
# "ollama" または "llama_cpp" を選択
LLM_BACKEND="ollama"

# Ollamaを使用する場合
OLLAMA_HOST="http://localhost:11434"

# Llama.cppを使用する場合 (ローカルモデルのパス)
# LAMA_CPP_MODEL_PATH="/path/to/your/model.gguf"

# --------------------------
# API Keys
# --------------------------
# Tavily Search APIを使用する場合
# TAVILY_API_KEY="your_tavily_api_key"